{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple Webpage Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0jgfyDqhjBr"
      },
      "source": [
        "#### **Multiple Webpage Scraping(BeautifulSoup and Regex)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7HpZ0ivhyZq"
      },
      "source": [
        "##### **Web Scraping - URLs and Email IDs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5_dVZMwhgH6"
      },
      "source": [
        "# importing required libraries\r\n",
        "\r\n",
        "import urllib.request\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "# URL o Scrap\r\n",
        "wiki = \"https://dlca.logcluster.org/display/public/DLCA/4.1+Nepal+Government+Contact+List\"\r\n",
        "\r\n",
        "# Query the website and return the html to the variable 'page'\r\n",
        "# For python 3 use urllib.request.urlopen(wiki)\r\n",
        "page = urllib.request.urlopen(wiki)\r\n",
        "\r\n",
        "# Parse the html in the 'page' variable, and store it in Beautiful Soup format\r\n",
        "soup = BeautifulSoup(page,features='html.parser')\r\n",
        "print('\\n\\nPage Scrapped !!!\\n\\n')\r\n",
        "\r\n",
        "print('\\n\\nTITLE OF THE PAGE\\n\\n')\r\n",
        "print(soup.title.string)\r\n",
        "\r\n",
        "print('\\n\\nALL THE URLs IN THE WEB PAGE\\n\\n')\r\n",
        "\r\n",
        "all_links = soup.find_all('a')\r\n",
        "\r\n",
        "\r\n",
        "print('Total number of URLs present = ',len(all_links))\r\n",
        "\r\n",
        "print('\\n\\nLast 5 URLs in the page are : \\n')\r\n",
        "\r\n",
        "if len(all_links) > 5:\r\n",
        "\r\n",
        "  last_5 = all_links[lens(all_links)-5:]\r\n",
        "  for url in last_5:\r\n",
        "    print(url.get('href'))\r\n",
        "\r\n",
        "\r\n",
        "emails = []\r\n",
        "for url in all_links:\r\n",
        "  if(str(url.get('href')).find('@') > 0):\r\n",
        "    emails.append(url.get('href'))\r\n",
        "\r\n",
        "\r\n",
        "print('\\n\\nTotal Number of Email IDs Present: ', len(emails))\r\n",
        "\r\n",
        "\r\n",
        "print('\\n\\nSome of the emails are: \\n\\n')\r\n",
        "for email in emails[:5]:\r\n",
        "  print(email)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}